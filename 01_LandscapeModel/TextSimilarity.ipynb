{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Text_Similarityipynb",
      "provenance": [],
      "collapsed_sections": [
        "9_AdGS1DjDA7"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN32Dg2H1c3Af/05QAIlwQK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpsyche/rememberly/blob/main/TextSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gsJSIYRhX7b"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkjWPEkHiCzy",
        "outputId": "ddd95aec-8a41-46af-80c9-cb064f252acc"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 49.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.5MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 59.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp36-none-any.whl size=103068 sha256=3eb713a28030f552f0aba80a65939107c383dfa9a24c99c5bdbf71c072dc6ead\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=a277919346fb63e6b8542cfa9035eccab3228ea85559c4094a298ae31c5f4b39\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x775cNuzhSpr"
      },
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import scipy.spatial\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_AdGS1DjDA7"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9VEIYw6j4Vx",
        "outputId": "973da380-0f3a-46ad-b8b3-2c29bd088c19"
      },
      "source": [
        "model = SentenceTransformer(\"stsb-roberta-base\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 461M/461M [00:26<00:00, 17.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxMl5vinj6DH"
      },
      "source": [
        "model = model.to(dev)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6EXUDpakbvT"
      },
      "source": [
        "# testing spacy\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.', \n",
        "    'The quick brown fox jumps over the lazy dog.']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TifoS6hkYBX"
      },
      "source": [
        "embeds = model.encode(sentences, convert_to_tensor=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojere9kHnaL3",
        "outputId": "6e11f759-08e9-4041-bee0-f0df0ffe5b0f"
      },
      "source": [
        "type(embeds)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeB3KqzIuyyb",
        "outputId": "38f5058c-85d6-4d29-9ccc-81147444a88d"
      },
      "source": [
        "distance.pdist(embeds, metric=\"cosine\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.50537506, 0.96602266, 0.97348109])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7jLY7opwoRn",
        "outputId": "35d554b8-8b33-475a-c814-42eccae634a7"
      },
      "source": [
        "F.cosine_similarity(embeds[2:3], embeds[1:2])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0265])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzh_nZra0dh0"
      },
      "source": [
        "text_units = [[\"this is one\", \"of the many\", \"text units\", \"we have\", \"to use\"],\n",
        "              [\"this is another\"],\n",
        "              [\"do you think\", \"this could be\", \"a third?\"]]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C90nyYRG0ogA"
      },
      "source": [
        "text_unit_embeds = [model.encode(i, convert_to_tensor=True) for i in text_units]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl5LllXd0tjr",
        "outputId": "cde35cff-4f3c-4e67-93e9-3e2b1fdfc0e1"
      },
      "source": [
        "[distance.pdist(i, metric=\"cosine\") for i in text_unit_embeds]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0.66904016, 0.7714568 , 0.3554559 , 0.61600814, 0.85547665,\n",
              "        0.498224  , 0.68915933, 0.94102719, 0.66680099, 0.66650351]),\n",
              " array([], dtype=float64),\n",
              " array([0.72241644, 0.63780705, 0.84130491])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTZkkoDooajW"
      },
      "source": [
        "# Semantic Text Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrHLnPFgrj2U"
      },
      "source": [
        "# doing something like this is important for speed but with GCP something else might\n",
        "# have to be done\n",
        "if torch.cuda.is_available():\n",
        "  dev = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  dev = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m4TvmZR_2jc"
      },
      "source": [
        "Note: SentenceTransformer(\"sbert_model_name\") will download and store the sbert model we choose each session we build a TextSimilarity class. This takes a while, but once a certain model is downloaded, it's easy to use with multiple instances of TextSimilarity\n",
        "\n",
        "Also note that SBERT takes a while to run since it's fairly big, and pdist is O(n^2), this will become a lot slower with longer reading cycles. On the order of sentences as reading cycles, I don't think it will be too bad, but this is also on the Colab GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45XBIHgroeBq"
      },
      "source": [
        "# export\n",
        "class TextSimilarity(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Computes embeddings for a pair of given sentences and calculates the cosine\n",
        "  similarity between them\n",
        "  \"\"\"\n",
        "  def __init__(self, sbert_model_name):\n",
        "    \"\"\"\n",
        "    sbert_model_name: sbert model to use, I think \"stsb-roberta-base\" is good\n",
        "    \"\"\"\n",
        "    super(TextSimilarity, self).__init__()\n",
        "    self.model = SentenceTransformer(sbert_model_name)\n",
        "    self.to(dev)\n",
        "\n",
        "  @staticmethod\n",
        "  def measure(embedded_reading_cycle):\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between all text unit embeddings in a reading cycle\n",
        "    reading_cycle: list of text unit embeddings\n",
        "    \"\"\"\n",
        "    # no customizable metric - only cosine, otherwise 1 - x doesn't make sense\n",
        "    similarities = 1 - scipy.spatial.distance.pdist(embedded_reading_cycle, metric=\"cosine\")\n",
        "    # convert to torch tensors\n",
        "    return torch.tensor(similarities)\n",
        "\n",
        "  def forward(self, reading_cycles):\n",
        "    \"\"\"\n",
        "    Computes cosine similarities between n sentences as a matrix\n",
        "    reading_cycles: list of lists of text unit strings\n",
        "    \"\"\"\n",
        "    # embeds is the list of text_unit embeddings for each reading cycle of shape\n",
        "    # (num_reading_cycles, num_text_units, embed_dim)\n",
        "    # where num_text_units varies per reading cycle, and embed_dim = 768 as per BERT\n",
        "    embeds = [self.model.encode(reading_cycle, convert_to_tensor=True) for reading_cycle in reading_cycles]\n",
        "    # given n text units in a given reading_cycle\n",
        "    # pdist will compute n * (n - 1) // 2 similarities, so measures is a list of shape\n",
        "    # of shape (num_reading_cycles, num_text_units * (num_text_units - 1) // 2)\n",
        "    measures = [self.measure(emb_reading_cycle) for emb_reading_cycle in embeds]\n",
        "    # return similarity measures and embeddings\n",
        "    return measures, embeds"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ldzDD9xb1P",
        "outputId": "f206a3a7-7f42-4ff7-b142-400d171d8798"
      },
      "source": [
        "# testing\n",
        "ts = TextSimilarity(\"stsb-roberta-base\") # takes a while to load\n",
        "print(ts([[\"this is one\", \"of the many\", \"text units\", \"we have\", \"to use\"],\n",
        "              [\"this is another\"],\n",
        "              [\"do you think\", \"this could be\", \"a third?\"]])[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([0.3310, 0.2285, 0.6445, 0.3840, 0.1445, 0.5018, 0.3108, 0.0590, 0.3332,\n",
            "        0.3335], dtype=torch.float64), tensor([], dtype=torch.float64), tensor([0.2776, 0.3622, 0.1587], dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3IAKaSAdK8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}